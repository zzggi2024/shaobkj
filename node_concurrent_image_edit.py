import os
import json
import time
import threading
import traceback
import base64
import io
import shutil
import random
import re
import torch
import numpy as np
from collections import deque
from urllib.parse import urlparse
import folder_paths
from PIL import Image, ImageOps
from server import PromptServer
from aiohttp import web
from concurrent.futures import ThreadPoolExecutor, as_completed
from comfy.utils import ProgressBar
import nodes
import node_helpers
import comfy

from .shaobkj_shared import (
    get_config_value,
    create_requests_session,
    disable_insecure_request_warnings,
    build_submit_timeout,
    post_json_with_retry,
    auth_headers_for_same_origin,
    resize_and_encode_image,
    resize_pil_long_side,
    extract_image_from_json,
    save_local_record,
    sanitize_text,
    update_async_task,
    get_all_async_tasks,
    pil_to_tensor,
    tensor_to_pil,
    crop_image_to_ratio,
    detect_subject_bbox,
)

def get_closest_aspect_ratio(width, height):
    ratios = {
        "1:1": 1.0,
        "16:9": 16/9,
        "9:16": 9/16,
        "4:3": 4/3,
        "3:4": 3/4,
        "3:2": 3/2,
        "2:3": 2/3,
        "21:9": 21/9,
        "9:21": 9/21
    }
    target = width / height
    closest_ratio = "1:1"
    min_diff = float('inf')
    
    for r_str, r_val in ratios.items():
        diff = abs(target - r_val)
        if diff < min_diff:
            min_diff = diff
            closest_ratio = r_str
            
    return closest_ratio

# ----------------------------------------------------------------------------
# Background Worker (Async Sender Logic)
# ----------------------------------------------------------------------------

def run_concurrent_task_internal(data):
    # Use provided task_id or generate new one
    task_id_local = data.get("task_id")
    if not task_id_local:
        task_id_local = f"task_{int(time.time())}_{random.randint(1000,9999)}"
    
    print(f"[ComfyUI-shaobkj] [Concurrent-Sender] Starting concurrent task {task_id_local}...")
    
    # Initial status update
    update_async_task(task_id_local, {
        "status": "running",
        "submitted_at": int(time.time()),
        "prompt": data.get("prompt", "")[:50] + "...",
        "type": "edit"
    })

    try:
        # Parse common params
        api_key = data.get("api_key")
        api_url_base = data.get("api_url", "https://yhmx.work")
        model = data.get("model", "gemini-3-pro-image-preview")
        use_proxy = data.get("use_proxy", False)
        resolution = data.get("resolution", "1k")
        prompt = data.get("prompt", "")
        aspect_ratio = data.get("aspect_ratio", "åŽŸå›¾1æ¯”ä¾‹")
        long_side = int(data.get("long_side", 1280))
        wait_time = int(data.get("wait_time", 0))
        seed_val = int(data.get("seed", 0))
        save_path_input = data.get("save_path", "")
        save_format_input = data.get("save_format", "JPEG (é»˜è®¤95%)")
        accept_mode = data.get("accept_mode", "æ™ºèƒ½æ¨¡å¼")
        subject_text = data.get("subject_text", "")

        if not api_key:
             raise ValueError("API Key is required")

        # Collect Images
        pil_images = []
        
        # Process Uploaded Image
        image_name = data.get("image_name")
        if image_name:
             try:
                 p = folder_paths.get_annotated_filepath(image_name)
                 if p and os.path.exists(p):
                     img = Image.open(p)
                     img = ImageOps.exif_transpose(img)
                     pil_images.append(img)
             except Exception as e:
                 print(f"[ComfyUI-shaobkj] [Concurrent-Sender] Error loading uploaded image: {e}")

        # Process Additional Uploads (from JS dynamic inputs)
        additional_images = data.get("additional_images", [])
        for item in additional_images:
            if isinstance(item, dict) and item.get("value"):
                 try:
                     p = folder_paths.get_annotated_filepath(item.get("value"))
                     if p and os.path.exists(p):
                         img = Image.open(p)
                         img = ImageOps.exif_transpose(img)
                         pil_images.append(img)
                 except Exception:
                     pass

        # Process Tensor Images (already PIL)
        if "tensor_images" in data:
            pil_images.extend(data["tensor_images"])

        if not pil_images:
             raise ValueError("No valid images found (Check uploads or connections).")

        first_image_ratio = None
        first_image_size = None
        if pil_images:
            w0, h0 = pil_images[0].size
            if w0 > 0 and h0 > 0:
                first_image_ratio = float(w0) / float(h0)
                first_image_size = (int(w0), int(h0))
        subject_crop_ratio = None
        if isinstance(subject_text, str) and subject_text.strip():
            if aspect_ratio == "åŽŸå›¾1æ¯”ä¾‹" and first_image_size is not None:
                subject_crop_ratio = get_closest_aspect_ratio(first_image_size[0], first_image_size[1])
            elif aspect_ratio and aspect_ratio != "Free":
                subject_crop_ratio = str(aspect_ratio)

        def get_target_size_local(res, ar, first_size):
            target_map = {"1k": 1024, "2k": 2048, "4k": 4096}
            target = target_map.get(str(res).lower(), 1024)
            ratio_str = str(ar)
            if ratio_str == "åŽŸå›¾1æ¯”ä¾‹" and first_size is not None:
                ratio_str = get_closest_aspect_ratio(first_size[0], first_size[1])
            if ratio_str == "åŽŸå›¾1æ¯”ä¾‹" or ratio_str == "Free":
                return target, target
            if ":" in ratio_str:
                try:
                    a, b = ratio_str.split(":", 1)
                    aw = float(a)
                    ah = float(b)
                    if aw > 0 and ah > 0:
                        r = aw / ah
                        if r >= 1.0:
                            w = target
                            h = max(1, int(round(target / r)))
                        else:
                            h = target
                            w = max(1, int(round(target * r)))
                        return int(w), int(h)
                except Exception:
                    pass
            return target, target

        def adjust_image_aspect(pil_image):
            if pil_image is None:
                return None
            if aspect_ratio != "åŽŸå›¾1æ¯”ä¾‹" or first_image_ratio is None:
                return pil_image
            target_w, target_h = get_target_size_local(resolution, aspect_ratio, first_image_size)
            if target_w <= 0 or target_h <= 0:
                return pil_image
            w, h = pil_image.size
            if w <= 0 or h <= 0:
                return pil_image
            target_ratio = float(target_w) / float(target_h)
            src_ratio = float(w) / float(h)
            if abs(src_ratio - target_ratio) > 0.001:
                if src_ratio > target_ratio:
                    new_w = max(1, int(round(h * target_ratio)))
                    left = max(0, int((w - new_w) // 2))
                    pil_image = pil_image.crop((left, 0, left + new_w, h))
                else:
                    new_h = max(1, int(round(w / target_ratio)))
                    top = max(0, int((h - new_h) // 2))
                    pil_image = pil_image.crop((0, top, w, top + new_h))
            if pil_image.size != (int(target_w), int(target_h)):
                pil_image = pil_image.resize((int(target_w), int(target_h)), Image.LANCZOS)
            return pil_image

        def encode_input_image(pil_image):
            if pil_image is None:
                return None, None
            img = resize_pil_long_side(pil_image, long_side)
            if subject_crop_ratio:
                detect_timeout = 30 if wait_time <= 0 else max(5, min(30, int(wait_time)))
                bbox = detect_subject_bbox(img, subject_text.strip(), api_url_base, api_key, use_proxy, detect_timeout)
                img = crop_image_to_ratio(img, subject_crop_ratio, bbox)
            if "PNG" in str(save_format_input):
                if img.mode != "RGBA":
                    img = img.convert("RGBA")
                buffered = io.BytesIO()
                img.save(buffered, format="PNG")
                return base64.b64encode(buffered.getvalue()).decode("utf-8"), "image/png"
            if img.mode != "RGB":
                img = img.convert("RGB")
            buffered = io.BytesIO()
            img.save(buffered, format="JPEG", quality=95)
            return base64.b64encode(buffered.getvalue()).decode("utf-8"), "image/jpeg"

        if model == "æ™ºèƒ½åŠ è½½":
            resolution_key = str(resolution).lower()
            if resolution_key == "2k":
                model = "gemini-3-pro-image-preview-2k"
            elif resolution_key == "4k":
                model = "gemini-3-pro-image-preview-4k"
            else:
                model = "gemini-3-pro-image-preview"

        # Prepare Request
        base_origin = str(api_url_base).rstrip("/")
        api_origin = urlparse(base_origin).netloc
        
        url = f"{base_origin}/v1beta/models/{model}:generateContent"
        # Fix: Remove Authorization header for Gemini to improve proxy compatibility (align with ModeHub)
        headers = {"Content-Type": "application/json", "x-goog-api-key": api_key}
        
        # Force generation instruction
        final_prompt = str(prompt) + "\n\n(Generate an image based on this description)"
        parts = [{"text": final_prompt}]
        
        image_b64_list = [] # Store for fallback
        
        for img in pil_images:
            try:
                b64_str, mime_type = encode_input_image(img)
                if b64_str:
                    image_b64_list.append(b64_str)
                    parts.append({
                        "inline_data": {
                            "mime_type": mime_type,
                            "data": b64_str
                        }
                    })
            except Exception as e:
                print(f"[ComfyUI-shaobkj] [Concurrent-Sender] Error encoding image: {e}")

        # Seed Logic
        safe_seed = seed_val
        if safe_seed < 0:
            safe_seed = random.randint(0, 2147483647)
        if safe_seed > 2147483647:
            safe_seed = safe_seed % 2147483647

        payload = {
            "contents": [{"role": "user", "parts": parts}],
            "generationConfig": {
                "temperature": 0.7, 
                "seed": safe_seed, 
                "responseModalities": ["TEXT", "IMAGE"]
            }
        }
        payload["generationConfig"]["imageConfig"] = {"imageSize": str(resolution).upper()}

        target_aspect_ratio = aspect_ratio
        if target_aspect_ratio == "åŽŸå›¾1æ¯”ä¾‹" and len(pil_images) > 0:
             # Calculate from first image (assuming image_1 is first in list)
             w, h = pil_images[0].size
             target_aspect_ratio = get_closest_aspect_ratio(w, h)
             print(f"[ComfyUI-shaobkj] Calculated aspect ratio from image 1 ({w}x{h}): {target_aspect_ratio}")

        if target_aspect_ratio != "åŽŸå›¾1æ¯”ä¾‹" and target_aspect_ratio != "Free":
            payload["generationConfig"]["imageConfig"]["aspectRatio"] = str(target_aspect_ratio)

        # Debug: Print Image Config to verify Aspect Ratio
        print(f"[ComfyUI-shaobkj] [Debug] Sending Image Config: {payload.get('generationConfig', {}).get('imageConfig', {})}")

        # Send Request
        disable_insecure_request_warnings()
        session, proxies = create_requests_session(bool(use_proxy))
        submit_timeout = build_submit_timeout(wait_time)
        
        def decode_b64_image(b64_str):
            try:
                image_data = base64.b64decode(b64_str)
                image = Image.open(io.BytesIO(image_data))
                if image.mode != "RGB":
                    image = image.convert("RGB")
                return image
            except Exception:
                return None

        def download_url_image(image_url):
            try:
                img_headers = auth_headers_for_same_origin(str(image_url), api_origin, {"Authorization": f"Bearer {api_key}"})
                img_res = session.get(image_url, verify=False, timeout=60, proxies=proxies, headers=img_headers)
                img_res.raise_for_status()
                image = Image.open(io.BytesIO(img_res.content))
                if image.mode != "RGB":
                    image = image.convert("RGB")
                return image
            except Exception:
                return None

        def extract_image_by_mode(res_json):
            if accept_mode == "æ™ºèƒ½æ¨¡å¼":
                return extract_image_from_json(res_json, session, proxies, api_key, api_origin, timeout_val=60)
            if not isinstance(res_json, dict):
                return None
            if accept_mode == "URL":
                data_list = res_json.get("data")
                if isinstance(data_list, list):
                    for item in data_list:
                        if isinstance(item, dict) and isinstance(item.get("url"), str):
                            img = download_url_image(item.get("url"))
                            if img:
                                return img
                choices = res_json.get("choices")
                if isinstance(choices, list) and choices:
                    content_text = choices[0].get("message", {}).get("content", "")
                    if isinstance(content_text, str) and content_text:
                        urls = re.findall(r"!\[.*?\]\((.*?)\)", content_text)
                        if not urls:
                            urls = re.findall(r"(https?://[^\s)]+)", content_text)
                        for u in urls:
                            if isinstance(u, str) and not u.lower().startswith("data:"):
                                img = download_url_image(u)
                                if img:
                                    return img
                candidates = res_json.get("candidates")
                if isinstance(candidates, list) and candidates:
                    for cand in candidates:
                        content = cand.get("content") if isinstance(cand, dict) else None
                        parts = content.get("parts") if isinstance(content, dict) else None
                        if not isinstance(parts, list):
                            continue
                        for part in parts:
                            if not isinstance(part, dict):
                                continue
                            text_content = part.get("text")
                            if isinstance(text_content, str) and text_content:
                                urls = re.findall(r"!\[.*?\]\((.*?)\)", text_content)
                                if not urls:
                                    urls = re.findall(r"(https?://[^\s)]+)", text_content)
                                for u in urls:
                                    if isinstance(u, str) and not u.lower().startswith("data:"):
                                        img = download_url_image(u)
                                        if img:
                                            return img
                return None
            if accept_mode == "B64":
                candidates = res_json.get("candidates")
                if isinstance(candidates, list) and candidates:
                    for cand in candidates:
                        content = cand.get("content") if isinstance(cand, dict) else None
                        parts = content.get("parts") if isinstance(content, dict) else None
                        if not isinstance(parts, list):
                            continue
                        for part in parts:
                            if not isinstance(part, dict):
                                continue
                            inline = part.get("inlineData") or part.get("inline_data")
                            if isinstance(inline, dict) and inline.get("data"):
                                img = decode_b64_image(inline.get("data"))
                                if img:
                                    return img
                            text_content = part.get("text")
                            if isinstance(text_content, str) and text_content:
                                m = re.search(r"data:image/[^;]+;base64,([a-zA-Z0-9+/=]+)", text_content)
                                if m:
                                    img = decode_b64_image(m.group(1))
                                    if img:
                                        return img
                data_list = res_json.get("data")
                if isinstance(data_list, list):
                    for item in data_list:
                        if isinstance(item, dict) and item.get("b64_json"):
                            img = decode_b64_image(item.get("b64_json"))
                            if img:
                                return img
                choices = res_json.get("choices")
                if isinstance(choices, list) and choices:
                    content_text = choices[0].get("message", {}).get("content", "")
                    if isinstance(content_text, str) and content_text:
                        m = re.search(r"data:image/[^;]+;base64,([a-zA-Z0-9+/=]+)", content_text)
                        if m:
                            img = decode_b64_image(m.group(1))
                            if img:
                                return img
                return None
            return None

        # A. Helper Functions for Fallback
        def try_openai_fallback():
            openai_base = base_origin[:-3] if base_origin.endswith("/v1") else base_origin
            openai_url = f"{openai_base}/v1/chat/completions"
            openai_headers = {"Content-Type": "application/json", "Authorization": f"Bearer {api_key}"}
            openai_content = [{"type": "text", "text": final_prompt}]
            for b64_str in image_b64_list:
                openai_content.append({"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{b64_str}"}})
            openai_payload = {
                "model": model,
                "messages": [{"role": "user", "content": openai_content}],
                "temperature": 0.7,
                "seed": safe_seed,
            }
            try:
                openai_resp = post_json_with_retry(
                    session,
                    openai_url,
                    headers=openai_headers,
                    payload=openai_payload,
                    timeout=submit_timeout,
                    proxies=proxies,
                    verify=False,
                )
                openai_resp.raise_for_status()
                openai_json = openai_resp.json()
                return extract_image_by_mode(openai_json)
            except Exception as e:
                print(f"[ComfyUI-shaobkj] [Concurrent-Sender] OpenAI Fallback failed: {e}")
                return None

        def get_task_id_from_headers(resp):
            headers_map = getattr(resp, "headers", {}) or {}
            tid = headers_map.get("X-Task-Id") or headers_map.get("Task-Id") or headers_map.get("task_id") or headers_map.get("task-id")
            if tid: return tid
            location = headers_map.get("Location") or headers_map.get("location")
            if isinstance(location, str) and location:
                m = re.search(r"/([^/]+)/?$", location.strip())
                if m: return m.group(1)
            return None

        print(f"[ComfyUI-shaobkj] {task_id_local}: Sending request...")
        
        # B. Request with 524 Handling and Retry
        response = post_json_with_retry(
            session,
            url,
            headers=headers,
            payload=payload,
            timeout=submit_timeout,
            proxies=proxies,
            verify=False
        )


        
        if response.status_code not in (200, 201, 202):
            print(f"[ComfyUI-shaobkj] [Concurrent-Sender] {task_id_local}: API Error Status: {response.status_code}")
            # Try to read error body for logging
            try:
                print(f"[ComfyUI-shaobkj] [Concurrent-Sender] Error Body: {response.text[:200]}")
            except Exception:
                pass
        
        response.raise_for_status()
        
        try:
            res_json = response.json()
        except (json.JSONDecodeError, ValueError):
             # Handle Empty Response Body
             raw_text = response.text
             if not raw_text or not raw_text.strip():
                 print(f"[ComfyUI-shaobkj] [Concurrent-Sender] {task_id_local}: Warning: Empty response body (HTTP {response.status_code})")
                 task_id = get_task_id_from_headers(response)
                 if task_id:
                     res_json = {"id": task_id}
                 else:
                     print(f"[ComfyUI-shaobkj] [Concurrent-Sender] {task_id_local}: Trying OpenAI fallback due to empty response")
                     extracted_img = try_openai_fallback()
                     if extracted_img:
                         res_json = {"status": "success", "fallback": True}
                     else:
                         raise RuntimeError(f"API Error: Empty response body (HTTP {response.status_code})")
             else:
                 raise

        # Verify response content
        extracted_img = None
        if "fallback" in res_json:
             pass # Already extracted
        else:
             if not res_json or (not res_json.get("candidates") and not res_json.get("id") and not res_json.get("name") and not res_json.get("data")):
                print(f"[ComfyUI-shaobkj] {task_id_local}: Warning - Empty or invalid response from API.")

             # Extract Result using shared helper
             extracted_img = extract_image_by_mode(res_json)
        
        remote_task_id = None
        
        if not extracted_img:
             remote_task_id = res_json.get("id") or res_json.get("task_id")
             if not remote_task_id and "name" in res_json:
                 # Operation name as ID
                 remote_task_id = res_json["name"]
             if not remote_task_id and "data" in res_json:
                 remote_task_id = res_json["data"].get("id") or res_json["data"].get("task_id")
             
             if not remote_task_id:
                 # Check headers one last time
                 remote_task_id = get_task_id_from_headers(response)

             if remote_task_id:
                 print(f"[ComfyUI-shaobkj] {task_id_local}: Polling remote task {remote_task_id}...")
                 poll_url = f"{url}/{remote_task_id}"
                 poll_timeout_val = 86400 if wait_time == 0 else wait_time
                 start_poll = time.time()
                 fail_count = 0
                 
                 while True:
                     if (time.time() - start_poll) > poll_timeout_val:
                         raise RuntimeError("Timeout polling")
                     
                     time.sleep(2)
                     try:
                        poll_resp = session.get(poll_url, headers=headers, params={"_t": int(time.time()*1000)}, verify=False, proxies=proxies, timeout=30)
                        fail_count = 0
                        if poll_resp.status_code == 200:
                            poll_json = poll_resp.json()
                            extracted_img = extract_image_by_mode(poll_json)
                            if extracted_img:
                                break
                            
                            status = poll_json.get("status") or poll_json.get("task_status")
                            if status in ["FAILED", "ERROR"]:
                                raise RuntimeError(f"Remote Task failed: {status}")
                     except Exception as e:
                         fail_count += 1
                         print(f"[ComfyUI-shaobkj] [Concurrent-Sender] Polling error: {e}")
                         if fail_count > 10:
                             raise

        # Save Result
        if extracted_img:
            final_img = adjust_image_aspect(extracted_img)
            # Determine Format
            save_params = {"format": "JPEG", "quality": 95}
            ext = ".jpg"
            
            if save_format_input and "PNG" in save_format_input:
                save_params = {"format": "PNG"}
                ext = ".png"
            elif save_format_input and "WEBP" in save_format_input:
                save_params = {"format": "WEBP", "lossless": True}
                ext = ".webp"

            # Determine Filename
            custom_filename = data.get("output_filename")
            if custom_filename:
                base_name = os.path.splitext(os.path.basename(str(custom_filename)))[0]
                filename = f"{base_name}{ext}"
            else:
                filename = f"concurrent_edit_{int(time.time())}_{random.randint(1000,9999)}{ext}"
            
            # Determine output directory
            out_dir = folder_paths.get_output_directory()
            if save_path_input and isinstance(save_path_input, str) and save_path_input.strip():
                custom_dir = save_path_input.strip()
                # Check if absolute or relative
                if not os.path.isabs(custom_dir):
                    custom_dir = os.path.join(out_dir, custom_dir)
                
                try:
                    os.makedirs(custom_dir, exist_ok=True)
                    out_dir = custom_dir
                except Exception as e:
                    print(f"[ComfyUI-shaobkj] [Concurrent-Sender] Failed to create custom dir {custom_dir}, using default. Error: {e}")

            out_path = os.path.join(out_dir, filename)
            
            # Check for overwrite and append suffix if needed
            counter = 1
            original_out_path = out_path
            original_base_name = os.path.splitext(filename)[0]
            
            while os.path.exists(out_path):
                new_filename = f"{original_base_name}_{counter}{ext}"
                out_path = os.path.join(out_dir, new_filename)
                filename = new_filename # Update filename variable for logging/return
                counter += 1
            
            final_img.save(out_path, **save_params)
            
            print(f"[ComfyUI-shaobkj] [Concurrent-Sender] {task_id_local}: Success! Saved to {out_path}")
            
            # Record success in async manager
            update_async_task(task_id_local, {
                "status": "success",
                "image_path": out_path,
                "completed_at": int(time.time())
            })
            
            # Also record in local log file
            save_local_record("Concurrent_Edit", str(remote_task_id or task_id_local), "Success", api_url_base)
            
            PromptServer.instance.send_sync("shaobkj.concurrent.success", {"task_id": task_id_local, "filename": filename, "path": out_path})
            
            return task_id_local # Return success ID
        else:
            brief = sanitize_text(json.dumps(res_json, ensure_ascii=False))
            raise RuntimeError(f"No image data found in response: {brief}")

    except Exception as e:
        err_msg = str(e)
        # Simplify common API errors
        if "401" in err_msg or "Unauthorized" in err_msg or "invalid_api_key" in err_msg:
             err_msg = "âŒ é”™è¯¯ï¼šAPI Key æ— æ•ˆæˆ–æœªæŽˆæƒ (401 Unauthorized)ã€‚è¯·æ£€æŸ¥æ‚¨çš„ API Key æ˜¯å¦æ­£ç¡®ã€‚"
        elif "404" in err_msg or "Not Found" in err_msg:
             err_msg = "âŒ é”™è¯¯ï¼šAPI åœ°å€æˆ–æ¨¡åž‹æœªæ‰¾åˆ° (404 Not Found)ã€‚è¯·æ£€æŸ¥ API åœ°å€å’Œæ¨¡åž‹åç§°ã€‚"
        elif "429" in err_msg or "Too Many Requests" in err_msg or "quota" in err_msg.lower():
             err_msg = "âŒ é”™è¯¯ï¼šAPI é…é¢è€—å°½æˆ–è¯·æ±‚è¿‡äºŽé¢‘ç¹ (429 Too Many Requests)ã€‚"
        elif "500" in err_msg or "Internal Server Error" in err_msg:
             err_msg = "âŒ é”™è¯¯ï¼šAPI æœåŠ¡ç«¯å†…éƒ¨é”™è¯¯ (500 Internal Server Error)ã€‚"
        elif "504" in err_msg or "Gateway Time-out" in err_msg:
             err_msg = "âŒ é”™è¯¯ï¼šè¯·æ±‚è¶…æ—¶ (504 Gateway Time-out)ã€‚æœåŠ¡å™¨å¤„ç†æ—¶é—´è¿‡é•¿ã€‚"
        elif "Total execution time exceeded limit" in err_msg:
             err_msg = f"âŒ é”™è¯¯ï¼šç­‰å¾…è¶…æ—¶ã€‚ä»»åŠ¡æ‰§è¡Œæ—¶é—´è¶…è¿‡äº†è®¾å®šçš„é™åˆ¶ã€‚"
        elif "Read timed out" in err_msg or "Connect timed out" in err_msg:
             err_msg = f"âŒ é”™è¯¯ï¼šç½‘ç»œè¿žæŽ¥è¶…æ—¶ã€‚ç½‘ç»œå“åº”æ…¢ã€‚"
             
        print(f"[ComfyUI-shaobkj] [Concurrent-Sender] {task_id_local}: {err_msg}")
        
        # Record failure in async manager
        update_async_task(task_id_local, {
            "status": "failed",
            "error": err_msg,
            "completed_at": int(time.time())
        })

        traceback.print_exc()
        # Raise to let the executor know it failed
        raise RuntimeError(err_msg)


# ----------------------------------------------------------------------------
# API Route (Backward Compatibility)
# ----------------------------------------------------------------------------

@PromptServer.instance.routes.post("/shaobkj/concurrent/submit")
async def api_concurrent_submit(request):
    try:
        json_data = await request.json()
        
        # Resolve image path
        image_name = json_data.get("image_name")
        if image_name:
            image_path = folder_paths.get_annotated_filepath(image_name)
            json_data["image_path"] = image_path
        
        # Start background thread
        t = threading.Thread(target=run_concurrent_task_internal, args=(json_data,))
        t.daemon = True
        t.start()
        
        return web.json_response({"status": "success", "message": "Task started in background"})
        
    except Exception as e:
        return web.json_response({"status": "error", "message": str(e)}, status=500)


# ----------------------------------------------------------------------------
# Node A: Sender (Async)
# ----------------------------------------------------------------------------

class Shaobkj_ConcurrentImageEdit_Sender:
    def __init__(self):
        pass

    @classmethod
    def INPUT_TYPES(s):
        api_key_default = get_config_value("API_KEY", "SHAOBKJ_API_KEY", "")
        return {
            "required": {
                "æç¤ºè¯": ("STRING", {"multiline": True, "dynamicPrompts": True, "tooltip": "ç¼–è¾‘æè¿°ï¼Œæ”¯æŒå¤šè¡Œï¼›æŽ¨èï¼šæ¯è¡Œä¸€æ¡æç¤ºè¯"}),
                "APIå¯†é’¥": ("STRING", {"default": api_key_default, "multiline": False, "tooltip": "æœåŠ¡ç«¯ API Keyï¼›æŽ¨èï¼šå¡«å†™æœ‰æ•ˆ Key"}),
                "APIåœ°å€": ("STRING", {"default": "https://yhmx.work", "multiline": False, "tooltip": "API åŸºç¡€åœ°å€ï¼›æŽ¨èï¼šhttps://yhmx.work"}),
                "æ¨¡åž‹é€‰æ‹©": (
                    [
                        "gemini-3-pro-image-preview",
                        "æ™ºèƒ½åŠ è½½",
                    ],
                    {"default": "gemini-3-pro-image-preview", "tooltip": "æ¨¡åž‹é€‰æ‹©æˆ–æ™ºèƒ½åŠ è½½ï¼›æŽ¨èï¼šgemini-3-pro-image-preview"},
                ),
                "ä½¿ç”¨ç³»ç»Ÿä»£ç†": ("BOOLEAN", {"default": True, "tooltip": "æ˜¯å¦ä½¿ç”¨ç³»ç»Ÿä»£ç†ï¼›æŽ¨èï¼šå¼€å¯"}),
                "åˆ†è¾¨çŽ‡": (["1k", "2k", "4k"], {"default": "1k", "tooltip": "è¾“å‡ºåˆ†è¾¨çŽ‡æ¡£ä½ï¼›æŽ¨èï¼š1k"}),
                "å›¾ç‰‡æ¯”ä¾‹": (["Free", "1:1", "16:9", "9:16", "4:3", "3:4", "3:2", "2:3", "21:9", "9:21", "åŽŸå›¾1æ¯”ä¾‹"], {"default": "åŽŸå›¾1æ¯”ä¾‹", "tooltip": "è¾“å‡ºç”»é¢æ¯”ä¾‹ï¼›æŽ¨èï¼šåŽŸå›¾1æ¯”ä¾‹"}),
                "æŽ¥æ”¶æ¨¡å¼": (["æ™ºèƒ½æ¨¡å¼", "URL", "B64"], {"default": "æ™ºèƒ½æ¨¡å¼", "tooltip": "API è¿”å›žå†…å®¹å¤„ç†æ–¹å¼ï¼›æŽ¨èï¼šæ™ºèƒ½æ¨¡å¼"}),
                "ä¸»ä½“æ–‡æœ¬": ("STRING", {"default": "", "multiline": False, "tooltip": "ä¸»ä½“è¯†åˆ«è£åˆ‡å…³é”®è¯ï¼›æŽ¨èï¼šç•™ç©º"}),
                "è¾“å…¥å›¾åƒ-é•¿è¾¹è®¾ç½®": (["1024", "1280", "1536"], {"default": "1280", "tooltip": "è¾“å…¥å›¾åƒé•¿è¾¹ç¼©æ”¾ï¼›æŽ¨èï¼š1280"}),
                "seed": ("INT", {"default": 0, "min": 0, "max": 2147483647, "tooltip": "éšæœºç§å­ï¼›æŽ¨èï¼š0"}),
                "Batchæ‹†åˆ†æ¨¡å¼": ("BOOLEAN", {"default": True, "tooltip": "æ˜¯å¦æ‹†åˆ†æ‰¹æ¬¡æäº¤ï¼›æŽ¨èï¼šå¼€å¯"}),
                "Batchå¯¹é½æ–¹å¼": (["å¾ªçŽ¯è¡¥å…¨(Max)", "è£åˆ‡å¯¹é½(Min)"], {"default": "å¾ªçŽ¯è¡¥å…¨(Max)", "tooltip": "æ‰¹æ¬¡å¯¹é½ç­–ç•¥ï¼›æŽ¨èï¼šå¾ªçŽ¯è¡¥å…¨(Max)"}),
                "ä¿å­˜è·¯å¾„": ("STRING", {"default": "Shaobkj_Concurrent", "multiline": False, "tooltip": "ç›¸å¯¹è¾“å‡ºç›®å½•çš„å­è·¯å¾„ï¼›æŽ¨èï¼šShaobkj_Concurrent"}),
                "ä¿å­˜æ ¼å¼": (["JPEG (é»˜è®¤95%)", "PNG (æ— æŸ)", "WEBP (æ— æŸ)"], {"default": "JPEG (é»˜è®¤95%)", "tooltip": "è¾“å‡ºä¿å­˜æ ¼å¼ï¼›æŽ¨èï¼šJPEG (é»˜è®¤95%)"}),
                "æœ€å¤§å¹¶å‘æ•°": ("INT", {"default": 5, "min": 1, "max": 20, "step": 1, "tooltip": "åŽå°æœ€å¤§åŒæ—¶æ‰§è¡Œä»»åŠ¡æ•°ï¼›æŽ¨èï¼š5"}),
                "å¹¶å‘é—´éš”": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 60.0, "step": 0.1, "tooltip": "æ‰¹é‡ä»»åŠ¡æäº¤é—´éš”(ç§’)ï¼›æŽ¨èï¼š1.0"}),
            },
            "optional": {
                 "æ–‡ä»¶åæ¥æº": ("STRING", {"forceInput": True, "multiline": False, "dynamicPrompts": False, "tooltip": "ç”¨äºŽè¾“å‡ºå‘½åçš„æ–‡ä»¶åæ¥æºï¼›æŽ¨èï¼šç•™ç©º"}),
                 "image_1": ("IMAGE", {"tooltip": "è¾“å…¥å›¾åƒ1ï¼›æŽ¨èï¼šè¿žæŽ¥å‚è€ƒå›¾"}),
                 "image_2": ("IMAGE", {"tooltip": "è¾“å…¥å›¾åƒ2ï¼›æŽ¨èï¼šå¯é€‰"}),
                 "image_3": ("IMAGE", {"tooltip": "è¾“å…¥å›¾åƒ3ï¼›æŽ¨èï¼šå¯é€‰"}),
                 "image_4": ("IMAGE", {"tooltip": "è¾“å…¥å›¾åƒ4ï¼›æŽ¨èï¼šå¯é€‰"}),
            },
            "hidden": {
                "unique_id": "UNIQUE_ID",
            }
        }

    INPUT_IS_LIST = True

    RETURN_TYPES = ("STRING", "STRING")
    RETURN_NAMES = ("APIå“åº”", "çŠ¶æ€")
    FUNCTION = "submit_task"
    CATEGORY = "ðŸ¤–shaobkj-APIbox"
    OUTPUT_NODE = True

    def submit_task(self, æç¤ºè¯, APIå¯†é’¥, APIåœ°å€, æ¨¡åž‹é€‰æ‹©, ä½¿ç”¨ç³»ç»Ÿä»£ç†, åˆ†è¾¨çŽ‡, å›¾ç‰‡æ¯”ä¾‹, æŽ¥æ”¶æ¨¡å¼, ä¸»ä½“æ–‡æœ¬, ä¿å­˜è·¯å¾„, seed, **kwargs):
        # Unwrap parameters because INPUT_IS_LIST = True wraps everything in lists
        # We assume common parameters are same for all items (take first), OR we should support batching them too.
        # For simplicity, let's take the first item for "global" settings, but support batching for Prompts and Images.
        
        def get_val(v, default=None):
            if isinstance(v, list) and len(v) > 0:
                return v[0]
            return v
        
        api_key_val = get_val(APIå¯†é’¥)
        api_url_val = get_val(APIåœ°å€)
        model_val = get_val(æ¨¡åž‹é€‰æ‹©)
        use_proxy_val = get_val(ä½¿ç”¨ç³»ç»Ÿä»£ç†)
        resolution_val = get_val(åˆ†è¾¨çŽ‡)
        # prompt is special, might be list of strings
        prompts_val = æç¤ºè¯ # Keep as list if it is list
        aspect_ratio_val = get_val(å›¾ç‰‡æ¯”ä¾‹)
        accept_mode_val = get_val(æŽ¥æ”¶æ¨¡å¼)
        subject_text_val = get_val(ä¸»ä½“æ–‡æœ¬)
        save_path_val = get_val(ä¿å­˜è·¯å¾„)
        
        # kwargs handling (also wrapped in lists)
        # We need to reconstruct kwargs to be clean
        clean_kwargs = {}
        for k, v in kwargs.items():
            if k == "unique_id": continue
            clean_kwargs[k] = v

        long_side_val = int(get_val(kwargs.get("è¾“å…¥å›¾åƒ-é•¿è¾¹è®¾ç½®", [1280])))
        wait_time_val = 0 # Default to infinite wait
        seed_val = int(get_val(seed))
        batch_split_val = get_val(kwargs.get("Batchæ‹†åˆ†æ¨¡å¼", [True]))
        batch_align_val = get_val(kwargs.get("Batchå¯¹é½æ–¹å¼", ["å¾ªçŽ¯è¡¥å…¨(Max)"]))
        submit_interval_val = float(get_val(kwargs.get("å¹¶å‘é—´éš”", [1.0])))
        max_workers_val = int(get_val(kwargs.get("æœ€å¤§å¹¶å‘æ•°", [5])))
        save_format_val = get_val(kwargs.get("ä¿å­˜æ ¼å¼", ["JPEG (é»˜è®¤95%)"]))
        filename_source_val = kwargs.get("æ–‡ä»¶åæ¥æº", None) # Keep as list
        
        # 0. Pre-check
        if not api_key_val or str(api_key_val).strip() == "":
            raise ValueError("âŒ é”™è¯¯ï¼šAPI Key ä¸èƒ½ä¸ºç©º")
        if not api_url_val or str(api_url_val).strip() == "":
            raise ValueError("âŒ é”™è¯¯ï¼šAPI åœ°å€ä¸èƒ½ä¸ºç©º")
        
        # 1. Prepare Base Data
        base_task_id = f"task_{int(time.time())}_{random.randint(1000,9999)}"
        
        base_data = {
            "api_key": api_key_val,
            "api_url": api_url_val,
            "model": model_val,
            "use_proxy": use_proxy_val,
            "resolution": resolution_val,
            "prompt": prompts_val, # Might be list
            "aspect_ratio": aspect_ratio_val,
            "accept_mode": accept_mode_val,
            "subject_text": subject_text_val,
            "long_side": long_side_val,
            "wait_time": wait_time_val,
            "seed": seed_val,
            "save_path": save_path_val,
            "save_format": save_format_val
        }

        # Helper to normalize inputs to a flat list of tensors
        def normalize_image_input(val):
            flat_list = []
            if isinstance(val, list):
                for item in val:
                    if isinstance(item, list):
                         flat_list.extend(normalize_image_input(item))
                    elif isinstance(item, torch.Tensor):
                        # item is [B,H,W,C]
                        for i in range(item.shape[0]):
                            flat_list.append(item[i]) # Store as [H,W,C] tensors
            elif isinstance(val, torch.Tensor):
                 for i in range(val.shape[0]):
                    flat_list.append(val[i])
            return flat_list

        # Helper to normalize generic list inputs (recursively flatten)
        def normalize_list_input(val):
            flat_list = []
            if isinstance(val, list):
                for item in val:
                    flat_list.extend(normalize_list_input(item))
            else:
                flat_list.append(val)
            return flat_list

        # 2. Logic Branch: Batch Split vs Single Request
        
        task_list = []
        
        if not batch_split_val:
            # --- Legacy Mode: All images in one request ---
            data = base_data.copy()
            # If prompt is list, join them or take first? Legacy usually takes one string.
            if isinstance(data["prompt"], list):
                data["prompt"] = data["prompt"][0] if data["prompt"] else ""
            
            data["task_id"] = base_task_id
            data["tensor_images"] = []
            
            # Collect Images (Flatten all batches)
            for k, v in clean_kwargs.items():
                if k.startswith("image_"):
                    tensors = normalize_image_input(v)
                    for t in tensors:
                        if t.dim() == 3:
                            pil_img = Image.fromarray(np.clip(255. * t.cpu().numpy(), 0, 255).astype(np.uint8))
                            data["tensor_images"].append(pil_img)
            
            # Handle Filename
            f_list = []
            if filename_source_val:
                if isinstance(filename_source_val, list):
                     f_list = [str(x) for x in filename_source_val]
                else:
                     f_list = [str(filename_source_val)]
            if f_list:
                data["output_filename"] = f_list[0]

            task_list.append(data)

        else:
            # --- Batch Split Mode: One request per aligned item ---
            # 1. Identify Inputs and Normalize them
            normalized_inputs = {}
            for k, v in clean_kwargs.items():
                if k.startswith("image_"):
                    normalized_inputs[k] = normalize_image_input(v)
            
            sorted_keys = sorted(normalized_inputs.keys()) # image_1, image_2...
            
            # 2. Determine Max Batch Size
            batch_sizes = [len(v) for v in normalized_inputs.values()]
            
            # Debugging Info
            debug_msg = "[Shaobkj-Debug] Batch Inputs: "
            for k, v in normalized_inputs.items():
                debug_msg += f"{k}={len(v)}, "
            
            # Handle prompt list
            prompts = []
            raw_prompts = normalize_list_input(prompts_val)
            for p in raw_prompts:
                 if p is not None:
                     prompts.append(str(p))
            
            if not prompts:
                prompts = [""] # Fallback

            if len(prompts) > 1:
                batch_sizes.append(len(prompts))
                debug_msg += f"Prompts={len(prompts)}"
            else:
                debug_msg += "Prompt=Single"
            
            # Handle filename source
            filename_list = []
            if filename_source_val:
                raw_list = normalize_list_input(filename_source_val)
                for item in raw_list:
                    if isinstance(item, str) and "\n" in item:
                        filename_list.extend([x.strip() for x in item.split("\n") if x.strip()])
                    elif item is not None and str(item).strip() != "":
                        filename_list.append(str(item))
                
                if filename_list:
                    batch_sizes.append(len(filename_list))
                    debug_msg += f"Filenames={len(filename_list)}"
                else:
                    debug_msg += "Filenames=None"
            
            if not batch_sizes:
                final_batch_size = 1
            elif batch_align_val == "è£åˆ‡å¯¹é½(Min)":
                final_batch_size = min(batch_sizes)
            else:
                final_batch_size = max(batch_sizes)

            print(f"{debug_msg} => Mode={batch_align_val} => Final Batch Size: {final_batch_size}")
            
            for i in range(final_batch_size):
                sub_task_id = f"{base_task_id}_{i}"
                sub_data = base_data.copy()
                sub_data["task_id"] = sub_task_id
                sub_data["tensor_images"] = []
                
                # Assign Prompt
                if prompts:
                    sub_data["prompt"] = prompts[i % len(prompts)]
                
                # Assign Filename
                if filename_list:
                    sub_data["output_filename"] = filename_list[i % len(filename_list)]
                
                # Assign Images (Aligned Slicing)
                for k in sorted_keys:
                    tensor_list = normalized_inputs[k]
                    if tensor_list:
                        idx = i % len(tensor_list)
                        t = tensor_list[idx]
                        if t.dim() == 3:
                            pil_img = Image.fromarray(np.clip(255. * t.cpu().numpy(), 0, 255).astype(np.uint8))
                            sub_data["tensor_images"].append(pil_img)
                
                task_list.append(sub_data)

        # 3. Execute Tasks in Background (Fire-and-Forget)
        
        total_tasks = len(task_list)
        print(f"[ComfyUI-shaobkj] Preparing to run {total_tasks} tasks in background...")
        
        def background_batch_monitor(tasks, max_workers, split_mode, interval):
            total_tasks = len(tasks)
            print(f"[ComfyUI-shaobkj-BG] [Concurrent-Sender] Background monitor started for {total_tasks} tasks. (Max Workers: {max_workers}, Interval: {interval}s)")
            
            success_count = 0
            fail_count = 0
            completed_count = 0
            failure_reasons = []
            lock = threading.Lock()
            
            def on_task_done(fut, tid):
                nonlocal success_count, fail_count, completed_count
                
                try:
                    fut.result()
                    is_success = True
                    error_msg = None
                except Exception as e:
                    is_success = False
                    error_msg = str(e)

                with lock:
                    completed_count += 1
                    if is_success:
                        success_count += 1
                        status_str = "å®Œæˆ"
                    else:
                        fail_count += 1
                        failure_reasons.append(f"{tid}: {error_msg}")
                        status_str = f"å¤±è´¥ï¼ŒåŽŸå› : {error_msg}"
                    
                    print(f"[ComfyUI-shaobkj-BG] [Concurrent-Sender] è¿›åº¦: {completed_count}/{total_tasks} | æˆåŠŸ: {success_count} | å¤±è´¥: {fail_count} | ä»»åŠ¡ {tid} {status_str}")

                    if completed_count == total_tasks:
                        summary = f"[ComfyUI-shaobkj-BG] [Concurrent-Sender] ä»»åŠ¡å·²å…¨éƒ¨å®Œæˆã€‚æ€»è®¡: {total_tasks} | æˆåŠŸ: {success_count} | å¤±è´¥: {fail_count}"
                        if fail_count > 0:
                            summary += f" | å¤±è´¥è¯¦æƒ…: {'; '.join(failure_reasons)}"
                        print(summary)

            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                for i, task_data in enumerate(tasks):
                    # Apply delay if needed
                    if split_mode and interval > 0 and i > 0:
                        time.sleep(interval)
                    
                    future = executor.submit(run_concurrent_task_internal, task_data)
                    # Use lambda to capture task_id immediately
                    future.add_done_callback(lambda f, t=task_data["task_id"]: on_task_done(f, t))

        # Launch Thread
        # We can use max_workers=5 or similar
        max_workers = max_workers_val
        if max_workers <= 0:
            max_workers = 1000 # "Unlimited" effectively
        
        t = threading.Thread(target=background_batch_monitor, args=(task_list, max_workers, batch_split_val, submit_interval_val))
        t.daemon = True
        t.start()
        
        msg = f"å·²æˆåŠŸæäº¤ {total_tasks} ä¸ªä»»åŠ¡åˆ°åŽå°è¿è¡Œã€‚\nè¯·æŸ¥çœ‹æŽ§åˆ¶å°æ—¥å¿—æˆ–è¾“å‡ºç›®å½•ç›‘æŽ§è¿›åº¦ã€‚\nå‰ç«¯å¯ç»§ç»­æ“ä½œã€‚"
        print(f"[ComfyUI-shaobkj] {msg}")
        
        # Return immediate feedback
        # Note: generated_ids is now just the list of IDs we *submitted*
        generated_ids = [t["task_id"] for t in task_list]
        status_list = ["Submitted (Background)" for _ in task_list]
        
        return (generated_ids, status_list)


# ----------------------------------------------------------------------------
# Node C: Load Batch Images From Path
# ----------------------------------------------------------------------------
class Shaobkj_Load_Image_Path:
    def __init__(self):
        pass

    @classmethod
    def INPUT_TYPES(s):
        input_dir = folder_paths.get_input_directory()
        files = [f for f in os.listdir(input_dir) if os.path.isfile(os.path.join(input_dir, f))]
        files = folder_paths.filter_files_content_types(files, ["image"])
        return {
            "required": {
                "image": (sorted(files), {"image_upload": True, "tooltip": "ä»Žè¾“å…¥ç›®å½•é€‰æ‹©å›¾åƒæ–‡ä»¶ï¼›æŽ¨èï¼šé€‰æ‹©ä¸€å¼ å›¾åƒ"}),
            }
        }

    RETURN_TYPES = ("IMAGE", "MASK", "STRING")
    RETURN_NAMES = ("image", "mask", "filename")
    FUNCTION = "load_image"
    CATEGORY = "ðŸ¤–shaobkj-APIbox/å®žç”¨å·¥å…·"

    def load_image(self, image):
        image_path = folder_paths.get_annotated_filepath(image)
        if not image_path or not os.path.exists(image_path):
            raise ValueError(f"âŒ é”™è¯¯ï¼šæ–‡ä»¶ä¸å­˜åœ¨: {image}")
        if not os.path.isfile(image_path):
            raise ValueError(f"âŒ é”™è¯¯ï¼šä¸æ˜¯æœ‰æ•ˆæ–‡ä»¶: {image}")

        img = Image.open(image_path)
        img = ImageOps.exif_transpose(img)

        if img.mode == "I":
            img = img.point(lambda i: i * (1 / 255))
        image = img.convert("RGB")
        image = np.array(image).astype(np.float32) / 255.0
        image = torch.from_numpy(image)[None,]

        if "A" in img.getbands():
            mask = np.array(img.getchannel("A")).astype(np.float32) / 255.0
            mask = 1.0 - torch.from_numpy(mask)
        else:
            mask = torch.zeros((image.shape[1], image.shape[2]), dtype=torch.float32, device="cpu")

        filename = os.path.basename(image_path)
        return (image, mask, filename)


class Shaobkj_Load_Batch_Images:
    def __init__(self):
        pass

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "directory": ("STRING", {"default": "", "multiline": False, "placeholder": "è¾“å…¥æ–‡ä»¶å¤¹è·¯å¾„ (å¦‚ C:\\images)", "tooltip": "å›¾ç‰‡æ‰€åœ¨æ–‡ä»¶å¤¹è·¯å¾„ï¼›æŽ¨èï¼šå¡«å†™æœ‰æ•ˆè·¯å¾„"}),
                "image_load_cap": ("INT", {"default": 0, "min": 0, "max": 10000, "step": 1, "tooltip": "é™åˆ¶åŠ è½½æ•°é‡ï¼Œ0ä¸ºä¸é™åˆ¶ï¼›æŽ¨èï¼š0"}),
                "start_index": ("INT", {"default": 0, "min": 0, "max": 10000, "step": 1, "tooltip": "ä»Žç¬¬å‡ ä¸ªæ–‡ä»¶å¼€å§‹åŠ è½½ï¼›æŽ¨èï¼š0"}),
                "load_always": ("BOOLEAN", {"default": False, "label_on": "enabled", "label_off": "disabled", "tooltip": "æ¯æ¬¡è¿è¡Œéƒ½é‡æ–°åŠ è½½ï¼›æŽ¨èï¼šå…³é—­"}),
                "sort_method": (["numerical", "alphabetical", "date"], {"default": "numerical", "tooltip": "æ–‡ä»¶æŽ’åºæ–¹å¼ï¼›æŽ¨èï¼šnumerical"}),
            }
        }

    RETURN_TYPES = ("IMAGE", "MASK", "STRING")
    RETURN_NAMES = ("images", "masks", "filenames")
    FUNCTION = "load_images"
    CATEGORY = "ðŸ¤–shaobkj-APIbox/å®žç”¨å·¥å…·"

    def load_images(self, directory, image_load_cap=0, start_index=0, load_always=False, sort_method="numerical"):
        folder_path = directory
        if not folder_path or not os.path.exists(folder_path):
             raise ValueError(f"âŒ é”™è¯¯ï¼šæ–‡ä»¶å¤¹è·¯å¾„ä¸å­˜åœ¨: {folder_path}")

        valid_extensions = {".jpg", ".jpeg", ".png", ".webp", ".bmp", ".tiff"}
        file_list = []
        
        try:
            for f in os.listdir(folder_path):
                ext = os.path.splitext(f)[1].lower()
                if ext in valid_extensions:
                    full_path = os.path.join(folder_path, f)
                    if os.path.isfile(full_path):
                        file_list.append(full_path)
        except Exception as e:
            raise ValueError(f"âŒ é”™è¯¯ï¼šè¯»å–æ–‡ä»¶å¤¹å¤±è´¥: {e}")

        if sort_method == "numerical":
             def natural_sort_key(s):
                 parts = re.split('([0-9]+)', s)
                 processed = []
                 for text in parts:
                     if text.isdigit():
                         processed.append((0, int(text)))
                     else:
                         cleaned = text.strip(" ()[]{}-_")
                         processed.append((1, cleaned.lower()))
                 return processed
             
             try:
                 file_list.sort(key=lambda x: natural_sort_key(os.path.basename(x)))
             except Exception as e:
                 print(f"[Shaobkj-Loader] Numerical sort failed: {e}. Fallback to default sort.")
                 file_list.sort()
        elif sort_method == "alphabetical":
             file_list.sort(key=lambda x: os.path.basename(x).lower())
        elif sort_method == "date":
             file_list.sort(key=lambda x: os.path.getmtime(x))
        else:
             file_list.sort()
        
        if start_index > 0:
            if start_index >= len(file_list):
                 file_list = []
            else:
                 file_list = file_list[start_index:]
        
        if image_load_cap > 0:
            file_list = file_list[:image_load_cap]

        if not file_list:
             raise ValueError(f"âŒ é”™è¯¯ï¼šæ–‡ä»¶å¤¹ä¸ºç©ºæˆ–ç­›é€‰åŽæ— æœ‰æ•ˆå›¾ç‰‡: {folder_path}")

        print(f"[Shaobkj-Loader] Found {len(file_list)} images in {folder_path}")
        
        images_out = []
        masks_out = []
        filenames_out = []
        
        for file_path in file_list:
            try:
                img = Image.open(file_path)
                img = ImageOps.exif_transpose(img)
                
                if img.mode == 'I':
                    img = img.point(lambda i: i * (1 / 255))
                image = img.convert("RGB")
                image = np.array(image).astype(np.float32) / 255.0
                image = torch.from_numpy(image)[None,]
                
                if 'A' in img.getbands():
                    mask = np.array(img.getchannel('A')).astype(np.float32) / 255.0
                    mask = 1. - torch.from_numpy(mask)
                else:
                    mask = torch.zeros((64,64), dtype=torch.float32, device="cpu")
                
                images_out.append(image)
                masks_out.append(mask)
                filenames_out.append(os.path.basename(file_path))
                
            except Exception as e:
                print(f"[Shaobkj-Loader] Error loading {file_path}: {e}")

        if not images_out:
             raise ValueError("No images loaded successfully.")

        return (images_out, masks_out, filenames_out)

class Shaobkj_Image_Save:
    def __init__(self):
        pass

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "å›¾åƒ": ("IMAGE", {"tooltip": "è¾“å…¥å›¾åƒï¼›æŽ¨èï¼šè¿žæŽ¥ä¸Šæ¸¸å›¾åƒè¾“å‡º"}),
                "ä¿å­˜è·¯å¾„": ("STRING", {"default": "Shaobkj_Save", "multiline": False, "tooltip": "ç›¸å¯¹è¾“å‡ºç›®å½•çš„å­è·¯å¾„ï¼›æŽ¨èï¼šShaobkj_Save"}),
                "ä¿å­˜æ ¼å¼": (["jpg", "pngï¼ˆé€æ˜Žåº•å›¾ï¼‰", "pngï¼ˆæ— æŸï¼‰"], {"default": "jpg", "tooltip": "ä¿å­˜æ ¼å¼ï¼›æŽ¨èï¼šjpg"}),
                "æ–‡ä»¶å": ("STRING", {"default": "image", "multiline": False, "tooltip": "ä¿å­˜æ–‡ä»¶å(ä¸å«æ‰©å±•å)ï¼›æŽ¨èï¼šimage"}),
                "è´¨é‡": ("INT", {"default": 100, "min": 1, "max": 100, "step": 1, "tooltip": "JPG è´¨é‡(1-100)ï¼›æŽ¨èï¼š100"}),
                "é¢„è§ˆ": ("BOOLEAN", {"default": True, "label_on": "å¼€å¯", "label_off": "å…³é—­", "tooltip": "æ˜¯å¦åœ¨ç•Œé¢æ˜¾ç¤ºé¢„è§ˆï¼›æŽ¨èï¼šå¼€å¯"}),
            }
        }

    RETURN_TYPES = ()
    RETURN_NAMES = ()
    FUNCTION = "save_image"
    CATEGORY = "ðŸ¤–shaobkj-APIbox/å®žç”¨å·¥å…·"
    OUTPUT_NODE = True

    def save_image(self, å›¾åƒ, ä¿å­˜è·¯å¾„, ä¿å­˜æ ¼å¼, æ–‡ä»¶å, è´¨é‡, é¢„è§ˆ):
        images = å›¾åƒ
        if isinstance(images, torch.Tensor) and images.dim() == 3:
            images = images.unsqueeze(0)

        output_root = folder_paths.get_output_directory()
        out_dir = output_root
        if isinstance(ä¿å­˜è·¯å¾„, str) and ä¿å­˜è·¯å¾„.strip():
            custom_dir = ä¿å­˜è·¯å¾„.strip()
            if not os.path.isabs(custom_dir):
                custom_dir = os.path.join(output_root, custom_dir)
            os.makedirs(custom_dir, exist_ok=True)
            out_dir = custom_dir

        fmt_label = ä¿å­˜æ ¼å¼ if isinstance(ä¿å­˜æ ¼å¼, str) else str(ä¿å­˜æ ¼å¼)
        is_jpg = fmt_label == "jpg"
        is_png_transparent = fmt_label == "pngï¼ˆé€æ˜Žåº•å›¾ï¼‰"
        is_png_lossless = fmt_label == "pngï¼ˆæ— æŸï¼‰"

        if is_jpg:
            save_params = {"format": "JPEG", "quality": int(è´¨é‡)}
            ext = ".jpg"
        elif is_png_transparent or is_png_lossless:
            save_params = {"format": "PNG"}
            ext = ".png"
        else:
            save_params = {"format": "JPEG", "quality": int(è´¨é‡)}
            ext = ".jpg"

        base_name = str(æ–‡ä»¶å).strip() if æ–‡ä»¶å is not None else ""
        base_name = os.path.splitext(os.path.basename(base_name))[0]
        if not base_name:
            base_name = "image"

        filenames = []
        out_paths = []
        for i in range(images.shape[0]):
            img_tensor = images[i]
            t = img_tensor
            if isinstance(t, torch.Tensor) and t.dim() == 4:
                t = t[0]
            if isinstance(t, torch.Tensor) and t.dim() == 3 and t.shape[0] in (1, 3, 4) and t.shape[-1] not in (1, 3, 4):
                t = t.permute(1, 2, 0)
            arr = t.detach().cpu().numpy() if isinstance(t, torch.Tensor) else np.array(t)
            arr = np.clip(arr, 0.0, 1.0)
            if arr.ndim == 2:
                arr = arr[:, :, None]
            if arr.shape[-1] == 1:
                arr = np.repeat(arr, 3, axis=2)
            if is_png_transparent:
                if arr.shape[-1] >= 4:
                    img_arr = (arr[:, :, :4] * 255.0).astype(np.uint8)
                    pil_img = Image.fromarray(img_arr, mode="RGBA")
                else:
                    alpha = np.ones((arr.shape[0], arr.shape[1], 1), dtype=arr.dtype)
                    rgba = np.concatenate([arr[:, :, :3], alpha], axis=2)
                    img_arr = (rgba * 255.0).astype(np.uint8)
                    pil_img = Image.fromarray(img_arr, mode="RGBA")
            else:
                if arr.shape[-1] >= 4:
                    rgb = arr[:, :, :3]
                    alpha = np.clip(arr[:, :, 3:4], 0.0, 1.0)
                    white = np.ones_like(rgb)
                    comp = np.clip(rgb + (1.0 - alpha) * white, 0.0, 1.0)
                    img_arr = (comp * 255.0).astype(np.uint8)
                else:
                    img_arr = (arr[:, :, :3] * 255.0).astype(np.uint8)
                pil_img = Image.fromarray(img_arr, mode="RGB")
            filename = f"{base_name}{ext}"
            out_path = os.path.join(out_dir, filename)
            counter = 1
            while os.path.exists(out_path):
                filename = f"{base_name}_{counter}{ext}"
                out_path = os.path.join(out_dir, filename)
                counter += 1
            pil_img.save(out_path, **save_params)
            filenames.append(filename)
            out_paths.append(out_path)

        if é¢„è§ˆ:
            preview_entries = []
            out_dir_in_output = False
            try:
                out_dir_in_output = os.path.commonpath([output_root, out_dir]) == os.path.abspath(output_root)
            except Exception:
                out_dir_in_output = False

            if out_dir_in_output:
                rel_path = os.path.relpath(out_dir, output_root)
                if rel_path == ".":
                    rel_path = ""
                preview_entries = [
                    {"filename": name, "subfolder": rel_path, "type": "output"}
                    for name in filenames
                ]
            else:
                preview_dir = os.path.join(output_root, "Shaobkj_Preview")
                os.makedirs(preview_dir, exist_ok=True)
                preview_subfolder = "Shaobkj_Preview"
                for saved_path, saved_name in zip(out_paths, filenames):
                    preview_name = saved_name
                    preview_path = os.path.join(preview_dir, preview_name)
                    counter = 1
                    base_preview = os.path.splitext(preview_name)[0]
                    ext_preview = os.path.splitext(preview_name)[1]
                    while os.path.exists(preview_path):
                        preview_name = f"{base_preview}_{counter}{ext_preview}"
                        preview_path = os.path.join(preview_dir, preview_name)
                        counter += 1
                    shutil.copyfile(saved_path, preview_path)
                    preview_entries.append({"filename": preview_name, "subfolder": preview_subfolder, "type": "output"})

            return {"ui": {"images": preview_entries}}
        return {}

class Shaobkj_FourWayRepair_HD:
    def __init__(self):
        pass

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "å›¾åƒ": ("IMAGE", {"tooltip": "è¾“å…¥å›¾åƒï¼›æŽ¨èï¼šå¾…ä¿®å¤å›¾åƒ"}),
                "æ¨¡åž‹": ("MODEL", {"tooltip": "ä½¿ç”¨çš„æ¨¡åž‹ï¼›æŽ¨èï¼šä¸Žå·¥ç¨‹ä¸€è‡´æ¨¡åž‹"}),
                "VAE": ("VAE", {"tooltip": "VAE æ¨¡åž‹ï¼›æŽ¨èï¼šä¸Žæ¨¡åž‹åŒ¹é…"}),
                "æ­£é¢æ¡ä»¶": ("CONDITIONING", {"tooltip": "æ­£é¢æç¤ºè¯æ¡ä»¶ï¼›æŽ¨èï¼šè¿žæŽ¥æ­£é¢æ¡ä»¶"}),
                "è´Ÿé¢æ¡ä»¶": ("CONDITIONING", {"tooltip": "è´Ÿé¢æç¤ºè¯æ¡ä»¶ï¼›æŽ¨èï¼šè¿žæŽ¥è´Ÿé¢æ¡ä»¶"}),
                "seed": ("INT", {"default": 0, "min": 0, "max": 0xffffffffffffffff, "tooltip": "éšæœºç§å­ï¼›æŽ¨èï¼š0"}),
                "steps": ("INT", {"default": 20, "min": 1, "max": 10000, "tooltip": "é‡‡æ ·æ­¥æ•°ï¼›æŽ¨èï¼š20"}),
                "cfg": ("FLOAT", {"default": 8.0, "min": 0.0, "max": 100.0, "step": 0.1, "tooltip": "CFG å¼•å¯¼å¼ºåº¦ï¼›æŽ¨èï¼š8.0"}),
                "é‡‡æ ·å™¨": (comfy.samplers.KSampler.SAMPLERS, {"tooltip": "é‡‡æ ·å™¨ç±»åž‹ï¼›æŽ¨èï¼šé»˜è®¤å€¼"}),
                "è°ƒåº¦å™¨": (comfy.samplers.KSampler.SCHEDULERS, {"tooltip": "è°ƒåº¦å™¨ç±»åž‹ï¼›æŽ¨èï¼šé»˜è®¤å€¼"}),
                "ä¿®è¡¥å¸¦å®½ç™¾åˆ†æ¯”": ("FLOAT", {"default": 0.15, "min": 0.01, "max": 0.5, "step": 0.01, "tooltip": "ä¿®è¡¥å¸¦å®½æ¯”ä¾‹ï¼›æŽ¨èï¼š0.15"}),
                "è½¯è¾¹æ¯”ä¾‹": ("FLOAT", {"default": 0.5, "min": 0.1, "max": 1.0, "step": 0.05, "tooltip": "ä¿®è¡¥è½¯è¾¹æ¯”ä¾‹ï¼›æŽ¨èï¼š0.5"}),
                "denoise": ("FLOAT", {"default": 0.75, "min": 0.0, "max": 1.0, "step": 0.01, "tooltip": "åŽ»å™ªå¼ºåº¦ï¼›æŽ¨èï¼š0.75"}),
                "å¯ç”¨åˆ†å—": ("BOOLEAN", {"default": True, "tooltip": "æ˜¯å¦å¯ç”¨åˆ†å—å¤„ç†ï¼›æŽ¨èï¼šå¼€å¯"}),
                "åˆ†å—å°ºå¯¸": ("INT", {"default": 1024, "min": 256, "max": 4096, "step": 64, "tooltip": "åˆ†å—å¤§å°ï¼›æŽ¨èï¼š1024"}),
                "åˆ†å—é‡å ": ("INT", {"default": 64, "min": 0, "max": 512, "step": 8, "tooltip": "åˆ†å—é‡å åƒç´ ï¼›æŽ¨èï¼š64"}),
                "å™ªå£°é®ç½©": ("BOOLEAN", {"default": True, "tooltip": "æ˜¯å¦ä½¿ç”¨å™ªå£°é®ç½©ï¼›æŽ¨èï¼šå¼€å¯"}),
            },
            "optional": {
                "é®ç½©": ("MASK", {"tooltip": "å¯é€‰é®ç½©ï¼›æŽ¨èï¼šéœ€è¦åŒºåŸŸæ—¶è¿žæŽ¥"}),
            }
        }

    RETURN_TYPES = ("IMAGE", "LATENT")
    RETURN_NAMES = ("å›¾åƒ", "latent")
    FUNCTION = "repair"
    CATEGORY = "ðŸ¤–shaobkj-APIbox"

    def repair(self, å›¾åƒ, æ¨¡åž‹, VAE, æ­£é¢æ¡ä»¶, è´Ÿé¢æ¡ä»¶, seed, steps, cfg, é‡‡æ ·å™¨, è°ƒåº¦å™¨, ä¿®è¡¥å¸¦å®½ç™¾åˆ†æ¯”, è½¯è¾¹æ¯”ä¾‹, denoise, å¯ç”¨åˆ†å—, åˆ†å—å°ºå¯¸, åˆ†å—é‡å , å™ªå£°é®ç½©=True, é®ç½©=None):
        def build_soft_band_mask(h, w, band_w, soft_ratio, axis, device):
            band_w = max(1, int(band_w))
            soft_ratio = float(soft_ratio)
            feather = max(1, int(round(band_w * soft_ratio)))
            if axis == "v":
                start = max(0, w // 2 - band_w // 2)
                end = min(w, start + band_w)
                x = torch.arange(w, device=device)
                dist = torch.where(x < start, start - x, torch.where(x >= end, x - (end - 1), torch.zeros_like(x)))
                mask_x = torch.clamp(1.0 - dist.float() / float(feather), 0.0, 1.0)
                return mask_x.view(1, w).repeat(h, 1)
            start = max(0, h // 2 - band_w // 2)
            end = min(h, start + band_w)
            y = torch.arange(h, device=device)
            dist = torch.where(y < start, start - y, torch.where(y >= end, y - (end - 1), torch.zeros_like(y)))
            mask_y = torch.clamp(1.0 - dist.float() / float(feather), 0.0, 1.0)
            return mask_y.view(h, 1).repeat(1, w)

        def inpaint_once(pixels_hw, mask_hw):
            pixels_bhwc = pixels_hw.unsqueeze(0)
            h_local, w_local = int(pixels_bhwc.shape[1]), int(pixels_bhwc.shape[2])
            mask_bchw = mask_hw.reshape((1, 1, h_local, w_local))
            x = (pixels_bhwc.shape[1] // 8) * 8
            y = (pixels_bhwc.shape[2] // 8) * 8
            mask_interp = torch.nn.functional.interpolate(mask_bchw, size=(pixels_bhwc.shape[1], pixels_bhwc.shape[2]), mode="bilinear").clamp(0.0, 1.0)
            orig_pixels = pixels_bhwc
            pixels = orig_pixels.clone()
            if pixels.shape[1] != x or pixels.shape[2] != y:
                x_offset = (pixels.shape[1] % 8) // 2
                y_offset = (pixels.shape[2] % 8) // 2
                pixels = pixels[:, x_offset:x + x_offset, y_offset:y + y_offset, :]
                mask_interp = mask_interp[:, :, x_offset:x + x_offset, y_offset:y + y_offset]
            m = (1.0 - mask_interp).squeeze(1)
            for i in range(3):
                pixels[:, :, :, i] -= 0.5
                pixels[:, :, :, i] *= m
                pixels[:, :, :, i] += 0.5
            concat_latent = VAE.encode(pixels)
            orig_latent = VAE.encode(orig_pixels)
            latent = {"samples": orig_latent}
            if å™ªå£°é®ç½©:
                latent["noise_mask"] = mask_interp
            pos = node_helpers.conditioning_set_values(æ­£é¢æ¡ä»¶, {"concat_latent_image": concat_latent, "concat_mask": mask_interp})
            neg = node_helpers.conditioning_set_values(è´Ÿé¢æ¡ä»¶, {"concat_latent_image": concat_latent, "concat_mask": mask_interp})
            sampled = nodes.common_ksampler(æ¨¡åž‹, int(seed), int(steps), float(cfg), é‡‡æ ·å™¨, è°ƒåº¦å™¨, pos, neg, latent, denoise=float(denoise))[0]
            decoded = VAE.decode(sampled["samples"])
            return decoded[0]

        def get_mask_input(h, w, device):
            if é®ç½© is None:
                return None
            m_in = é®ç½©
            if isinstance(m_in, torch.Tensor) and m_in.dim() == 4:
                m_in = m_in[0, 0] if m_in.shape[1] == 1 else m_in[0]
            if isinstance(m_in, torch.Tensor) and (m_in.shape[0] != h or m_in.shape[1] != w):
                m_in = torch.nn.functional.interpolate(m_in.reshape((1, 1, m_in.shape[-2], m_in.shape[-1])), size=(h, w), mode="bilinear").squeeze(0).squeeze(0)
            return m_in.to(device=device, dtype=torch.float32)

        def build_tile_indices(length, tile, overlap):
            tile = max(1, int(tile))
            overlap = max(0, int(overlap))
            if tile >= length:
                return [0]
            stride = max(1, tile - overlap)
            indices = []
            pos = 0
            while True:
                if pos + tile >= length:
                    indices.append(max(0, length - tile))
                    break
                indices.append(pos)
                pos += stride
            return indices

        def build_tile_weight(h, w, y0, x0, tile_h, tile_w, overlap):
            overlap = max(0, int(overlap))
            if overlap == 0:
                return torch.ones((tile_h, tile_w), device=img.device, dtype=torch.float32)
            wx = torch.ones((tile_w,), device=img.device, dtype=torch.float32)
            wy = torch.ones((tile_h,), device=img.device, dtype=torch.float32)
            if x0 > 0:
                ramp = torch.linspace(0.0, 1.0, steps=min(overlap, tile_w), device=img.device)
                wx[:ramp.shape[0]] = ramp
            if x0 + tile_w < w:
                ramp = torch.linspace(1.0, 0.0, steps=min(overlap, tile_w), device=img.device)
                wx[-ramp.shape[0]:] = torch.minimum(wx[-ramp.shape[0]:], ramp)
            if y0 > 0:
                ramp = torch.linspace(0.0, 1.0, steps=min(overlap, tile_h), device=img.device)
                wy[:ramp.shape[0]] = ramp
            if y0 + tile_h < h:
                ramp = torch.linspace(1.0, 0.0, steps=min(overlap, tile_h), device=img.device)
                wy[-ramp.shape[0]:] = torch.minimum(wy[-ramp.shape[0]:], ramp)
            return wy.view(tile_h, 1) * wx.view(1, tile_w)

        def inpaint_tiled(pixels_hw, mask_full, tile_size, overlap):
            h, w = int(pixels_hw.shape[0]), int(pixels_hw.shape[1])
            ys = build_tile_indices(h, tile_size, overlap)
            xs = build_tile_indices(w, tile_size, overlap)
            acc = torch.zeros_like(pixels_hw)
            acc_w = torch.zeros((h, w), device=pixels_hw.device, dtype=torch.float32)
            for y0 in ys:
                for x0 in xs:
                    y1 = min(h, y0 + tile_size)
                    x1 = min(w, x0 + tile_size)
                    tile = pixels_hw[y0:y1, x0:x1, :]
                    mask_tile = mask_full[y0:y1, x0:x1]
                    tile_out = inpaint_once(tile, mask_tile)
                    weight = build_tile_weight(h, w, y0, x0, tile.shape[0], tile.shape[1], overlap)
                    acc[y0:y1, x0:x1, :] += tile_out * weight.unsqueeze(-1)
                    acc_w[y0:y1, x0:x1] += weight
            acc = acc / acc_w.clamp(min=1e-6).unsqueeze(-1)
            return acc

        images = å›¾åƒ
        if isinstance(images, torch.Tensor) and images.dim() == 3:
            images = images.unsqueeze(0)
        batch = []
        latents = []
        for img in images:
            if isinstance(img, torch.Tensor) and img.dim() == 4:
                img = img[0]
            img = torch.clamp(img, 0.0, 1.0)
            h, w = int(img.shape[0]), int(img.shape[1])
            bw = max(1, int(round(min(h, w) * float(ä¿®è¡¥å¸¦å®½ç™¾åˆ†æ¯”))))
            shifted = torch.roll(img, shifts=(h // 2, w // 2), dims=(0, 1))
            img_stage = shifted
            mask_in = get_mask_input(h, w, img.device)
            mask_v = build_soft_band_mask(h, w, bw, è½¯è¾¹æ¯”ä¾‹, "v", img.device)
            if mask_in is not None:
                mask_v = torch.maximum(mask_v, mask_in)
            mask_h = build_soft_band_mask(h, w, bw, è½¯è¾¹æ¯”ä¾‹, "h", img.device)
            if mask_in is not None:
                mask_h = torch.maximum(mask_h, mask_in)
            if å¯ç”¨åˆ†å— and max(h, w) > int(åˆ†å—å°ºå¯¸):
                tile = int(åˆ†å—å°ºå¯¸)
                overlap = int(åˆ†å—é‡å )
                img_stage = inpaint_tiled(img_stage, mask_v, tile, overlap)
                img_stage = inpaint_tiled(img_stage, mask_h, tile, overlap)
            else:
                img_stage = inpaint_once(img_stage, mask_v)
                img_stage = inpaint_once(img_stage, mask_h)
            out = torch.roll(img_stage, shifts=(-h // 2, -w // 2), dims=(0, 1))
            out = torch.clamp(out, 0.0, 1.0)
            batch.append(out)
            latent_samples = VAE.encode(out.unsqueeze(0))
            latents.append(latent_samples)
        latent_out = {"samples": torch.cat(latents, dim=0)} if latents else {"samples": torch.empty((0,))}
        return (torch.stack(batch, dim=0), latent_out)

class Shaobkj_Fixed_Seed:
    def __init__(self):
        pass

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "seed": ("INT", {"default": 0, "min": 0, "max": 0xffffffffffffffff, "tooltip": "å›ºå®šéšæœºç§å­ï¼›æŽ¨èï¼š0"})
            }
        }

    RETURN_TYPES = ("INT",)
    RETURN_NAMES = ("seed",)
    FUNCTION = "get_seed"
    CATEGORY = "ðŸ¤–shaobkj-APIbox"

    def get_seed(self, seed):
        return (int(seed),)

    @classmethod
    def IS_CHANGED(cls, seed):
        return seed

# ----------------------------------------------------------------------------
# Node Registration
# ----------------------------------------------------------------------------

NODE_CLASS_MAPPINGS = {
    "Shaobkj_ConcurrentImageEdit_Sender": Shaobkj_ConcurrentImageEdit_Sender,
    "Shaobkj_Load_Image_Path": Shaobkj_Load_Image_Path,
    "Shaobkj_Load_Batch_Images": Shaobkj_Load_Batch_Images,
    "Shaobkj_Image_Save": Shaobkj_Image_Save,
    "Shaobkj_FourWayRepair_HD": Shaobkj_FourWayRepair_HD,
    "Shaobkj_Fixed_Seed": Shaobkj_Fixed_Seed
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "Shaobkj_ConcurrentImageEdit_Sender": "ðŸ¤–å¹¶å‘-ç¼–è¾‘-å›¾åƒé©±åŠ¨",
    "Shaobkj_Load_Image_Path": "ðŸ¤–åŠ è½½å›¾åƒ",
    "Shaobkj_Load_Batch_Images": "ðŸ¤–æ‰¹é‡åŠ è½½å›¾ç‰‡ (Path)",
    "Shaobkj_Image_Save": "ðŸ¤–å›¾åƒä¿å­˜",
    "Shaobkj_FourWayRepair_HD": "ðŸ¤–å››æ–¹ä¿®å¤é«˜æ¸…",
    "Shaobkj_Fixed_Seed": "ðŸ¤–å›ºå®šéšæœºç§å­"
}

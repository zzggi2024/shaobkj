import json
import re
import io
import base64
import traceback
import torch
import requests
from urllib.parse import urlparse
from server import PromptServer
from .shaobkj_shared import (
    get_config_value,
    post_json_with_retry,
    create_requests_session,
    disable_insecure_request_warnings,
    build_submit_timeout,
    resize_and_encode_image,
    resize_pil_long_side,
    tensor_to_pil
)

class Shaobkj_LLM_App:
    def __init__(self):
        pass

    @classmethod
    def INPUT_TYPES(s):
        api_key_default = get_config_value("API_KEY", "SHAOBKJ_API_KEY", "")
        return {
            "required": {
                "APIå¯†é’¥": ("STRING", {"default": api_key_default, "multiline": False, "tooltip": "æœåŠ¡ç«¯ API Keyï¼›æ¨èï¼šå¡«å†™æœ‰æ•ˆ Key"}),
                "APIåœ°å€": ("STRING", {"default": "https://yhmx.work", "multiline": False, "tooltip": "API åŸºç¡€åœ°å€ï¼›æ¨èï¼šhttps://yhmx.work"}),
                "æ¨¡å‹é€‰æ‹©": (["gemini-2.5-flash", "gemini-3-pro-preview"], {"default": "gemini-2.5-flash", "tooltip": "æ¨¡å‹é€‰æ‹©ï¼›æ¨èï¼šgemini-2.5-flash"}),
                "ä½¿ç”¨ç³»ç»Ÿä»£ç†": ("BOOLEAN", {"default": True, "tooltip": "æ˜¯å¦ä½¿ç”¨ç³»ç»Ÿä»£ç†ï¼›æ¨èï¼šå¼€å¯"}),
                "ç³»ç»ŸæŒ‡ä»¤": ("STRING", {"default": "ä½ æ˜¯é«˜æ•ˆçš„AIæç¤ºè¯ç”Ÿæˆå¤§å¸ˆã€‚è¯·æ ¹æ®ç”¨æˆ·è¾“å…¥ç”Ÿæˆå¯ç›´æ¥æ‰§è¡Œçš„æ–¹æ¡ˆæˆ–å†…å®¹ï¼Œç»“æ„æ¸…æ™°ï¼Œç›´æ¥è¾“å‡ºæç¤ºè¯ï¼Œä¸è¦æœ‰ä»»ä½•åºŸè¯ã€‚", "multiline": True, "tooltip": "ç³»ç»Ÿçº§æŒ‡ä»¤ï¼›æ¨èï¼šé»˜è®¤å†…å®¹"}),
                "ç”¨æˆ·è¾“å…¥": ("STRING", {"default": "", "multiline": True, "tooltip": "ç”¨æˆ·è¾“å…¥å†…å®¹ï¼›æ¨èï¼šæ¸…æ™°å…·ä½“"}),
                "æ€è€ƒæ¨¡å¼": ("BOOLEAN", {"default": False, "label_on": "å¼€å¯", "label_off": "å…³é—­", "tooltip": "æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼ï¼›æ¨èï¼šå…³é—­"}),
                "æ€è€ƒé¢„ç®—": ("INT", {"default": 10240, "min": 1024, "max": 65536, "step": 1024, "tooltip": "æ€è€ƒé¢„ç®—ä¸Šé™ï¼›æ¨èï¼š10240"}),
                "temperature": ("FLOAT", {"default": 0.7, "min": 0.0, "max": 2.0, "step": 0.1, "tooltip": "é‡‡æ ·æ¸©åº¦ï¼›æ¨èï¼š0.7"}),
                "topP": ("FLOAT", {"default": 0.95, "min": 0.0, "max": 1.0, "step": 0.01, "tooltip": "æ ¸é‡‡æ ·æ¦‚ç‡ï¼›æ¨èï¼š0.95"}),
                "è¾“å…¥å›¾åƒ-é•¿è¾¹è®¾ç½®": (["1024", "1280", "1536"], {"default": "1280", "tooltip": "è¾“å…¥å›¾åƒé•¿è¾¹ç¼©æ”¾ï¼›æ¨èï¼š1280"}),
                "ç­‰å¾…æ—¶é—´": ("INT", {"default": 180, "min": 0, "max": 1000000, "tooltip": "è¯·æ±‚è¶…æ—¶æ—¶é—´(ç§’)ï¼›æ¨èï¼š180"}),
                "seed": ("INT", {"default": 0, "min": 0, "max": 2147483647, "tooltip": "éšæœºç§å­ï¼›æ¨èï¼š0"}),
                "APIç”³è¯·åœ°å€": ("STRING", {"default": "https://yhmx.work/login?expired=true", "multiline": False, "tooltip": "API ç”³è¯·å…¥å£ï¼›æ¨èï¼šé»˜è®¤åœ°å€"}),
            },
        }

    RETURN_TYPES = ("STRING", "STRING")
    RETURN_NAMES = ("æ–‡æœ¬å†…å®¹", "APIå“åº”")
    FUNCTION = "run_llm"
    CATEGORY = "ğŸ¤–shaobkj-APIbox"

    def run_llm(self, APIå¯†é’¥, APIåœ°å€, æ¨¡å‹é€‰æ‹©, ä½¿ç”¨ç³»ç»Ÿä»£ç†, ç³»ç»ŸæŒ‡ä»¤, ç”¨æˆ·è¾“å…¥, æ€è€ƒæ¨¡å¼, æ€è€ƒé¢„ç®—, temperature, topP, è¾“å…¥å›¾åƒ_é•¿è¾¹è®¾ç½®=1280, ç­‰å¾…æ—¶é—´=180, seed=0, **kwargs):
        # Implementation of LLM Logic
        return self._execute_llm(APIå¯†é’¥, APIåœ°å€, æ¨¡å‹é€‰æ‹©, ä½¿ç”¨ç³»ç»Ÿä»£ç†, ç³»ç»ŸæŒ‡ä»¤, ç”¨æˆ·è¾“å…¥, æ€è€ƒæ¨¡å¼, æ€è€ƒé¢„ç®—, temperature, topP, è¾“å…¥å›¾åƒ_é•¿è¾¹è®¾ç½®, ç­‰å¾…æ—¶é—´, seed, **kwargs)

    def _execute_llm(self, APIå¯†é’¥, APIåœ°å€, æ¨¡å‹é€‰æ‹©, ä½¿ç”¨ç³»ç»Ÿä»£ç†, ç³»ç»ŸæŒ‡ä»¤, ç”¨æˆ·è¾“å…¥, æ€è€ƒæ¨¡å¼, æ€è€ƒé¢„ç®—, temperature, topP, è¾“å…¥å›¾åƒ_é•¿è¾¹è®¾ç½®, ç­‰å¾…æ—¶é—´, seed, **kwargs):
        api_key = APIå¯†é’¥
        if not api_key:
            raise ValueError("API Key is required.")

        base_origin = str(APIåœ°å€).rstrip("/")
        if base_origin.endswith("/v1"):
            base_origin = base_origin[:-3]
            
        model = æ¨¡å‹é€‰æ‹©
        long_side_val = int(kwargs.get("è¾“å…¥å›¾åƒ-é•¿è¾¹è®¾ç½®", è¾“å…¥å›¾åƒ_é•¿è¾¹è®¾ç½®))
        image_inputs = []
        for k, v in kwargs.items():
            if k.startswith("image_") and v is not None:
                image_inputs.append((k, v))
        image_inputs.sort(key=lambda x: int(x[0].split("_")[1]))

        def append_images(parts):
            for _, tensor in image_inputs:
                try:
                    if isinstance(tensor, torch.Tensor) and tensor.dim() == 4:
                        for i in range(tensor.shape[0]):
                            pil_img = tensor_to_pil(tensor[i])
                            b64_str, _ = resize_and_encode_image(pil_img, long_side_val)
                            if b64_str:
                                parts.append({"inline_data": {"mime_type": "image/jpeg", "data": b64_str}})
                    else:
                        pil_img = tensor_to_pil(tensor)
                        b64_str, _ = resize_and_encode_image(pil_img, long_side_val)
                        if b64_str:
                            parts.append({"inline_data": {"mime_type": "image/jpeg", "data": b64_str}})
                except Exception:
                    pass

        # ... (Rest of the LLM logic is reused, we can extract this common logic if needed, but for now I will duplicate the core request logic to support both nodes) ...
        # For brevity and safety, I will keep the original implementation here and just call it.
        # However, since I cannot easily call the method from another class without inheritance or a helper, 
        # I will refactor the core request logic into a standalone function or keep it inside.
        
        # To avoid massive code duplication, let's just use the logic I read previously.
        # Since I'm rewriting the file, I'll include the full logic.
        
        url = f"{base_origin}/v1beta/models/{model}:generateContent"
        # ... (Logic for Gemini 2.5 Flash and others) ...
        
        # Actually, let's implement the full logic cleanly.
        
        disable_insecure_request_warnings()
        session, proxies = create_requests_session(bool(ä½¿ç”¨ç³»ç»Ÿä»£ç†))
        submit_timeout = build_submit_timeout(int(ç­‰å¾…æ—¶é—´))

        # Construct Prompt
        system_prompt = ç³»ç»ŸæŒ‡ä»¤.strip() if isinstance(ç³»ç»ŸæŒ‡ä»¤, str) else ""
        user_prompt = ç”¨æˆ·è¾“å…¥.strip()
        
        # Special handling for Gemini 2.5 Flash (often doesn't support systemInstruction field well in some proxy/versions, so prepending is safer)
        # But for 3-pro-preview, systemInstruction is supported.
        # Let's use the standard payload structure.
        
        parts = [{"text": user_prompt}]
        append_images(parts)
        
        payload = {
            "contents": [{"role": "user", "parts": parts}],
            "generationConfig": {
                "temperature": temperature,
                "topP": topP,
                "seed": int(seed) if int(seed) >= 0 else 0
            }
        }
        
        if system_prompt:
             payload["systemInstruction"] = {"parts": [{"text": system_prompt}]}

        if æ€è€ƒæ¨¡å¼:
             payload["generationConfig"]["thinkingConfig"] = {
                "includeThoughts": True,
                "thinkingBudget": æ€è€ƒé¢„ç®—
            }

        headers = {"Content-Type": "application/json", "x-goog-api-key": api_key}
        
        try:
            response = post_json_with_retry(
                session,
                url,
                headers=headers,
                payload=payload,
                timeout=submit_timeout,
                proxies=proxies,
                verify=False,
            )
            
            if response.status_code != 200:
                return (f"API Error {response.status_code}: {response.text}", str(response.text))

            res_json = response.json()
            
            generated_text = ""
            if "candidates" in res_json and len(res_json["candidates"]) > 0:
                candidate = res_json["candidates"][0]
                if "content" in candidate and "parts" in candidate["content"]:
                    for part in candidate["content"]["parts"]:
                        if "text" in part:
                            generated_text += part["text"]
            
            if not generated_text:
                generated_text = "No text response generated."
                
            return (generated_text, json.dumps(res_json, ensure_ascii=False))

        except Exception as e:
            return (f"Error: {str(e)}", str(traceback.format_exc()))


class Shaobkj_NanoBanana_Prompt:
    def __init__(self):
        pass

    @classmethod
    def INPUT_TYPES(s):
        api_key_default = get_config_value("API_KEY", "SHAOBKJ_API_KEY", "")
        return {
            "required": {
                "APIå¯†é’¥": ("STRING", {"default": api_key_default, "multiline": False, "tooltip": "æœåŠ¡ç«¯ API Key"}),
                "APIåœ°å€": ("STRING", {"default": "https://yhmx.work", "multiline": False, "tooltip": "API åŸºç¡€åœ°å€"}),
                "ç”¨æˆ·è¾“å…¥": ("STRING", {"default": "", "multiline": True, "tooltip": "æè¿°ä½ æƒ³ç”Ÿæˆçš„ç”»é¢ï¼Œä¾‹å¦‚ï¼šåˆ¶ä½œä¸€å¼ xxå“ç‰Œé¦™æ°´çš„å±•ç¤ºæµ·æŠ¥"}),
                "ä»»åŠ¡ç±»å‹": (["ç”Ÿæˆæ¨¡å¼ (Generation)", "ç¼–è¾‘æ¨¡å¼ (Editing)"], {"default": "ç”Ÿæˆæ¨¡å¼ (Generation)", "tooltip": "é€‰æ‹©ç”Ÿæˆæç¤ºè¯è¿˜æ˜¯ç¼–è¾‘æŒ‡ä»¤"}),
                "åœºæ™¯é£æ ¼": (
                    ["Editorial (æ‚å¿—å¤§ç‰‡)", "Cinematic (ç”µå½±è´¨æ„Ÿ)", "Product Shot (äº§å“ç‰¹å†™)", "Minimalist (æç®€ä¸»ä¹‰)", "None (ä¸æŒ‡å®š)"], 
                    {"default": "Editorial (æ‚å¿—å¤§ç‰‡)", "tooltip": "ä»…åœ¨ç”Ÿæˆæ¨¡å¼ä¸‹ç”Ÿæ•ˆ"}
                ),
                "å“ç‰Œåç§°": ("STRING", {"default": "", "multiline": False, "tooltip": "å¯é€‰ï¼šæ›¿æ¢æç¤ºè¯ä¸­çš„ [BRAND]"}),
                "ä½¿ç”¨ç³»ç»Ÿä»£ç†": ("BOOLEAN", {"default": True, "tooltip": "æ˜¯å¦ä½¿ç”¨ç³»ç»Ÿä»£ç†"}),
                "seed": ("INT", {"default": 0, "min": 0, "max": 2147483647}),
            },
            "optional": {
                "åŸå›¾æè¿°": ("STRING", {"default": "", "multiline": True, "tooltip": "ä»…åœ¨ç¼–è¾‘æ¨¡å¼ä¸‹ç”Ÿæ•ˆï¼šç®€è¦æè¿°åŸå›¾å†…å®¹"}),
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("æç¤ºè¯",)
    FUNCTION = "generate_prompt"
    CATEGORY = "ğŸ¤–shaobkj-APIbox"

    def generate_prompt(self, APIå¯†é’¥, APIåœ°å€, ç”¨æˆ·è¾“å…¥, ä»»åŠ¡ç±»å‹, åœºæ™¯é£æ ¼, å“ç‰Œåç§°, ä½¿ç”¨ç³»ç»Ÿä»£ç†, seed, åŸå›¾æè¿°=""):
        # 1. Select System Prompt based on Task Type
        if "Editing" in ä»»åŠ¡ç±»å‹:
            system_prompt = """You are an expert AI image editor using Nano Banana Pro. Convert the user's request into a precise, direct editing instruction.
Rules:
1. Use imperative verbs (Replace, Remove, Change, Add, Keep).
2. Be specific about what to change and what to keep.
3. For background changes: 'Replace the background with [new scene]...'
4. For restoration: 'Restore this image, remove scratches, enhance details...'
5. Output ONLY the English instruction, no explanations."""
            user_content = f"Request: {ç”¨æˆ·è¾“å…¥}\nOriginal Image Context: {åŸå›¾æè¿°}"
        else:
            style_guide = ""
            if "Editorial" in åœºæ™¯é£æ ¼:
                style_guide = "Style: High-end fashion editorial, Vogue-style photography, studio lighting."
            elif "Cinematic" in åœºæ™¯é£æ ¼:
                style_guide = "Style: Cinematic movie still, dramatic lighting, anamorphic lens flare, color graded."
            elif "Product" in åœºæ™¯é£æ ¼:
                style_guide = "Style: Professional product photography, macro details, sharp focus, commercial lighting."
            elif "Minimalist" in åœºæ™¯é£æ ¼:
                style_guide = "Style: Minimalist composition, clean lines, negative space, soft pastel colors."

            system_prompt = f"""You are an expert prompt engineer for Nano Banana Pro (Gemini 3 Pro Image).
Your task is to convert the user's simple description into a high-quality, hyper-realistic prompt following this structure:
[Concept & Vibe] -> [Composition & Angle] -> [Subject Details] -> [Lighting & Atmosphere] -> [Texture & Realism].

Reference Example:
"A hyper-realistic editorial concept for a collaboration between [BRAND] and [MAGAZINE BRAND]. Square 1:1 composition, shot in a sleek Parisian interior with marble floors and tall windows, golden afternoon light illuminating the scene. A single model in a couture gown poses gracefully beside a realistically sized [BRAND] perfume bottle with the [BRAND] logo clearly visible placed on a marble pedestal. Ultra-refined textures, cinematic realism, Vogue-style photography."

Instructions:
1. {style_guide}
2. If the user provides a brand name, use it naturally.
3. Output ONLY the final English prompt.
4. Enhance details for realism (8k, detailed texture, volumetric lighting)."""
            user_content = f"User Description: {ç”¨æˆ·è¾“å…¥}\nBrand Name: {å“ç‰Œåç§°}"

        # 2. Call LLM (Reusing Shaobkj_LLM_App logic wrapper or simple request)
        # To avoid dependency issues, we implement a lightweight request here.
        
        api_key = APIå¯†é’¥
        if not api_key:
            return ("Error: API Key is required.",)

        base_origin = str(APIåœ°å€).rstrip("/")
        if base_origin.endswith("/v1"):
            base_origin = base_origin[:-3]
        
        url = f"{base_origin}/v1beta/models/gemini-2.5-flash:generateContent"
        headers = {"Content-Type": "application/json", "x-goog-api-key": api_key}
        
        payload = {
            "contents": [{"role": "user", "parts": [{"text": user_content}]}],
            "systemInstruction": {"parts": [{"text": system_prompt}]},
            "generationConfig": {"temperature": 0.7, "seed": int(seed)}
        }
        
        disable_insecure_request_warnings()
        session, proxies = create_requests_session(bool(ä½¿ç”¨ç³»ç»Ÿä»£ç†))
        
        try:
            response = post_json_with_retry(
                session, url, headers=headers, payload=payload, timeout=60, proxies=proxies, verify=False
            )
            if response.status_code != 200:
                return (f"Error: {response.text}",)
            
            res_json = response.json()
            generated_text = ""
            if "candidates" in res_json and len(res_json["candidates"]) > 0:
                candidate = res_json["candidates"][0]
                if "content" in candidate and "parts" in candidate["content"]:
                    for part in candidate["content"]["parts"]:
                        if "text" in part:
                            generated_text += part["text"]
            
            # Post-processing: Replace [BRAND] placeholder if needed (though LLM should have handled it)
            if å“ç‰Œåç§° and "[BRAND]" in generated_text:
                generated_text = generated_text.replace("[BRAND]", å“ç‰Œåç§°)
                
            return (generated_text.strip(),)
            
        except Exception as e:
            return (f"Error: {str(e)}",)
